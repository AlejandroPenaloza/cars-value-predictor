{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import re\n",
    "import requests\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_agg\n",
    "import matplotlib.figure\n",
    "import seaborn as sb\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML soups from all 500 web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soups(website_number):\n",
    "    get_url = requests.get('https://www.truecar.com/used-cars-for-sale/listings/?page=' + str(website_number))\n",
    "    return BeautifulSoup(get_url.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = list(map(get_soups, list(range(1, 8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_page_urls = np.array([])\n",
    "\n",
    "for ind in range(33):\n",
    "    finding = soups[0].find_all('a', {'data-test': 'usedListing'})[ind]\n",
    "    fst_page_urls = np.append(fst_page_urls, re.findall('href=\".+\" style', str(finding)[:280]))\n",
    "\n",
    "def urls_scraper(soup):\n",
    "    nth_urls = []\n",
    "    def urlsppage(nth):\n",
    "        finding = soup.find_all('a', {'data-test': 'usedListing'})[nth]\n",
    "        return re.findall('href=\"/.+\" style', str(finding)[:280])[0]\n",
    "    nth_urls = list(map(urlsppage, list(range(30))))\n",
    "    return nth_urls\n",
    "\n",
    "rest_urls_list = list(map(urls_scraper, soups[1:]))\n",
    "rest_pages_urls = np.array(rest_urls_list).flatten()\n",
    "all_urls = np.append(fst_page_urls, rest_pages_urls)\n",
    "url_formatter = np.vectorize(lambda url: 'https://truecar.com' + url[6: -7])\n",
    "urls = url_formatter(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(feature_as_argument):\n",
    "    \n",
    "    def feature_from_url(url):\n",
    "        nth_request = requests.get(url)\n",
    "        nth_soup = BeautifulSoup(nth_request.content, 'lxml')\n",
    "        nth_search = re.search(feature_as_argument + '</h4><ul><li>.+</li', str(nth_soup))\n",
    "        try:\n",
    "            return re.findall('li>.+</l', str(nth_search))[0][3: -3]\n",
    "        except:\n",
    "            return np.NaN\n",
    "    \n",
    "    return list(map(feature_from_url, urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First part of vehicles features scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_types = scraper('Drive Type')\n",
    "fuel_types = scraper('Fuel Type')\n",
    "mileages = scraper('Mileage')\n",
    "transmissions = scraper('Transmission')\n",
    "MPGs = scraper('MPG')\n",
    "styles = scraper('Style')\n",
    "options_levels = scraper('Options Level')\n",
    "bed_lengths = scraper('Bed Length')\n",
    "engines = scraper('Engine')\n",
    "exterior_colors = scraper('Exterior Color')\n",
    "interior_colors = scraper('Interior Color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicles Years, Makes and Models scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper2(index):\n",
    "    \n",
    "    def feature_from_url(url):\n",
    "        nth_request = requests.get(url)\n",
    "        nth_soup = BeautifulSoup(nth_request.content, 'lxml')\n",
    "        nth_finding = nth_soup.find_all('div', {'class': 'text-truncate heading-3 margin-right-2 margin-right-sm-3'})\n",
    "        try:\n",
    "            if index == 2:\n",
    "                return re.findall('>.+<', str(nth_search))[0][1: -1].split()[2:]\n",
    "            else:\n",
    "                return re.findall('>.+<', str(nth_search))[0][1: -1].split()[index]\n",
    "        except:\n",
    "            return np.NaN\n",
    "        \n",
    "    return list(map(feature_from_url, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = scraper2(0)\n",
    "makes = scraper2(1)\n",
    "models = scraper2(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicles Prices scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prices_scraper(url):\n",
    "    nth_request = requests.get(url).content\n",
    "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('div', {'data-qa': 'LabelBlock-text'})\n",
    "    try:\n",
    "        return re.findall('[0-9]+,[0-9]+', str(nth_soup))[0]\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = list(map(prices_scraper, urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicles Locations (Cities and States) scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cities_scraper(url):\n",
    "    nth_request = requests.get(url).content\n",
    "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('span', {'data-qa': 'used-vdp-header-location'})\n",
    "    try:\n",
    "        return re.findall('\">.+<!', str(nth_soup))[0][2: -12]\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_scraper(url):\n",
    "    nth_request = requests.get(url).content\n",
    "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('span', {'data-qa': 'used-vdp-header-location'})\n",
    "    try:\n",
    "        return re.findall('[A-W][A-Z]', str(nth_soup))[0]\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = list(map(cities_scraper, urls))\n",
    "states = list(map(states_scraper, urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicles Conditions scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditions_scraper(url):\n",
    "    nth_request = requests.get(url).content\n",
    "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('li', {'class': '_h9wfdq'})\n",
    "    try:\n",
    "        return re.findall('\">[0-9]<!', str(nth_soup[0]))[0][2: -2] + re.findall('->.+</l', str(nth_soup[0]))[0][2: -3]\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = list(map(conditions_scraper, urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'Make': makes, 'Model': models, 'Year': years, 'Mileage': mileages, 'Transmission': transmissions,\n",
    "    'Engine': engines, 'Exterior Color': exterior_colors, 'Interior Color': interior_colors,\n",
    "    'MPG': MPGs, 'Fuel Type': fuel_types, 'Drive Type': drive_types, 'Location (City)': cities,\n",
    "    'Location (State)': states, 'Style': styles, 'Condition (Accidents)': conditions,\n",
    "    'Options Level': options_levels, 'Bed Length': bed_lengths, 'Price': prices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_data = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
