{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "data-setting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTcNqQW71NsA"
      },
      "source": [
        "# Data collecting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on2yFBuX1VOY"
      },
      "source": [
        "### Importing libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1lN75ct1Nrj"
      },
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "import math\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWBxLcbm1NsH"
      },
      "source": [
        "### HTML soups from all 500 web pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3vEqtHj1NsL"
      },
      "source": [
        "def get_soups(website_number):\n",
        "    get_url = requests.get('https://www.truecar.com/used-cars-for-sale/listings/?page=' + str(website_number))\n",
        "    return BeautifulSoup(get_url.content, 'lxml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TahsuXDG1NsO"
      },
      "source": [
        "soups = list(map(get_soups, list(range(564))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYRbJn2x1NsP"
      },
      "source": [
        "## URLs scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaEzY-w71NsR"
      },
      "source": [
        "def urls_scraper(soup):\n",
        "    nth_urls = []\n",
        "    \n",
        "    def urlsppage(nth):\n",
        "        try:\n",
        "            finding = soup.find_all('a', {'data-test': 'usedListing'})[nth]\n",
        "            return re.findall('href=\"/.+\" style', str(finding)[:280])[0]\n",
        "        except:\n",
        "            return 0\n",
        "        \n",
        "    nth_urls = list(map(urlsppage, list(range(30))))\n",
        "    return nth_urls\n",
        "\n",
        "urls_list = list(map(urls_scraper, soups))\n",
        "pages_urls = np.array(urls_list).flatten()\n",
        "filter = [bool(url) for url in pages_urls if url != 0]\n",
        "fpages_urls = pages_urls[filter]\n",
        "url_formatter = np.vectorize(lambda url: 'https://truecar.com' + url[6: -7])\n",
        "urls = url_formatter(fpages_urls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2xcRfK11NsT"
      },
      "source": [
        "def fscraper(url, feature):\n",
        "    nth_request = requests.get(url)\n",
        "    nth_soup = BeautifulSoup(nth_request.content, 'lxml')\n",
        "    nth_search = re.search(feature + '</h4><ul><li>.+</li', str(nth_soup))\n",
        "    try:\n",
        "        return re.findall('li>.+</l', str(nth_search))[0][3: -3]\n",
        "    except:\n",
        "        return np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tePmXedU1NsU"
      },
      "source": [
        "## First vehicles features scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri1hE7be1NsV"
      },
      "source": [
        "drive_types = list(map(fscraper, urls, itertools.repeat('Drive Type')))\n",
        "fuel_types = list(map(fscraper, urls, itertools.repeat('Fuel Type')))\n",
        "mileages = list(map(fscraper, urls, itertools.repeat('Mileage')))\n",
        "transmissions = list(map(fscraper, urls, itertools.repeat('Transmission')))\n",
        "MPGs = list(map(fscraper, urls, itertools.repeat('MPG')))\n",
        "options_levels = list(map(fscraper, urls, itertools.repeat('Options Level')))\n",
        "bed_lengths = list(map(fscraper, urls, itertools.repeat('Bed Length')))\n",
        "engines = list(map(fscraper, urls, itertools.repeat('Engine')))\n",
        "exterior_colors = list(map(fscraper, urls, itertools.repeat('Exterior Color')))\n",
        "interior_colors = list(map(fscraper, urls, itertools.repeat('Interior Color')))\n",
        "styles = list(map(fscraper, urls, itertools.repeat('Style')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tLK-iog1NsX"
      },
      "source": [
        "## Vehicles Years, Makes and Models scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmdeRlAB1Nsa"
      },
      "source": [
        "def ymm_scraper(url, feat_name):\n",
        "    nth_request = requests.get(url)\n",
        "    nth_soup = BeautifulSoup(nth_request.content, 'lxml')\n",
        "    nth_finding = nth_soup.find_all('div', {'class': 'text-truncate heading-3 margin-right-2 margin-right-sm-3'})\n",
        "    try:\n",
        "        if feat_name == 'year':\n",
        "            return re.findall('>.+</', str(nth_finding))[0].split()[0][1:]\n",
        "        elif feat_name == 'make':\n",
        "            return ' '.join(re.findall('>.+</', str(nth_finding))[0].split()[1: -1])\n",
        "        else:\n",
        "            return re.findall('>.+</', str(nth_finding))[0].split()[-1][: -2]\n",
        "    except:\n",
        "        return np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIpWItQi1Nsb"
      },
      "source": [
        "years = list(map(ymm_scraper, urls, itertools.repeat('year')))\n",
        "makes = list(map(ymm_scraper, urls, itertools.repeat('make')))\n",
        "models = list(map(ymm_scraper, urls, itertools.repeat('model')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujfjpEIv1Nse"
      },
      "source": [
        "## Vehicles Prices scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wqYdDBQ1Nsf"
      },
      "source": [
        "def prices_scraper(url):\n",
        "    nth_request = requests.get(url).content\n",
        "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('div', {'data-qa': 'LabelBlock-text'})\n",
        "    try:\n",
        "        return re.findall('[0-9]+,[0-9]+', str(nth_soup))[0]\n",
        "    except:\n",
        "        return np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHodrnUd1Nsg"
      },
      "source": [
        "prices = list(map(prices_scraper, urls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw1JJ96a1Nsh"
      },
      "source": [
        "## Vehicles Locations (Cities and States) scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M213emDE1Nsj"
      },
      "source": [
        "def cities_scraper(url):\n",
        "    nth_request = requests.get(url).content\n",
        "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('span', {'data-qa': 'used-vdp-header-location'})\n",
        "    try:\n",
        "        return re.findall('\">.+<!', str(nth_soup))[0][2: -12]\n",
        "    except:\n",
        "        return np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldhi_pSb1Nsk"
      },
      "source": [
        "def states_scraper(url):\n",
        "    nth_request = requests.get(url).content\n",
        "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('span', {'data-qa': 'used-vdp-header-location'})\n",
        "    try:\n",
        "        return re.findall('[A-W][A-Z]', str(nth_soup))[0]\n",
        "    except:\n",
        "        return np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmjpkh3V1Nsl"
      },
      "source": [
        "cities = list(map(cities_scraper, urls))\n",
        "states = list(map(states_scraper, urls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NkamHfu1Nsm"
      },
      "source": [
        "## Vehicles Conditions scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc19bX0f1Nsn"
      },
      "source": [
        "def conditions_scraper(url):\n",
        "    nth_request = requests.get(url).content\n",
        "    nth_soup = BeautifulSoup(nth_request, 'lxml').find_all('li', {'class': '_h9wfdq'})\n",
        "    try:\n",
        "        return re.findall('\">[0-9]<!', str(nth_soup[0]))[0][2: -2] + re.findall('->.+</l', str(nth_soup[0]))[0][2: -3]\n",
        "    except:\n",
        "        return np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMbP4Vwz1Nso"
      },
      "source": [
        "conditions = list(map(conditions_scraper, urls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBv_XiWu1Nsq"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-X5EDu91Nsq"
      },
      "source": [
        "features = {\n",
        "    'Make': makes, 'Model': models, 'Year': years, 'Mileage': mileages, 'Transmission': transmissions,\n",
        "    'Engine': engines, 'Exterior Color': exterior_colors, 'Interior Color': interior_colors,\n",
        "    'MPG': MPGs, 'Fuel Type': fuel_types, 'Drive Type': drive_types, 'Location (City)': cities,\n",
        "    'Location (State)': states, 'Style': styles, 'Condition (Accidents)': conditions,\n",
        "    'Options Level': options_levels, 'Bed Length': bed_lengths, 'Price': prices\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfQEzi631Nsr"
      },
      "source": [
        "vehicles_data = pd.DataFrame(features)\n",
        "vehicles_data.to_csv('C:/.../.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9U264Ph1Nsr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}